Define paths to your training and testing data directories
Convert lists to NumPy arrays for easier processing
This project focuses on training a convolutional neural network (CNN) for image classification, leveraging structured data preprocessing, augmentation, and evaluation techniques to achieve high accuracy.

1. **Code Environment**:
   - Libraries used:
     - TensorFlow/Keras for model training and evaluation.
     - NumPy for numerical computations.
     - OpenCV for image processing.
     - Matplotlib for accuracy and loss visualization.
     - Sci-kit Learn for dataset splitting and performance evaluation.

2. **Data Preprocessing**:
   - **Resizing**: All images standardized to `(32x32)` pixels for input compatibility.
   - **Normalization**: Pixel values scaled between `[0,1]` for optimized training.
   - **Label Encoding**: Labels transformed into categorical format using TensorFlow utilities.
   - **Splitting**: 20% of the dataset allocated for validation.

 Model Architecture
- Convolutional Layers:
  - Three layers with filter sizes `(32, 64, 128)`.
  - Kernel size `(3x3)` and ReLU activation for feature extraction.
- Pooling Layers:
  - MaxPooling `(2x2)` applied after each convolutional layer.
- Dense Layers:
  - Flattening converts feature maps into 1D vectors.
  - Dense layer with 128 neurons and ReLU activation.
  - Dropout (0.5) applied to prevent overfitting.
- **Output Layer**:
  - Softmax activation for multi-class classification.

The graph illustrates model accuracy trends over **10 epochs**. The **blue** line represents train accuracy, while the **orange** line represents validation accuracy. Accuracy remained consistently high across epochs, stabilizing at **100% validation accuracy**.


Final Evaluation
**Final Test Accuracy: 100%  
Final Test Loss: Minimal

 Observations
- Validation accuracy remained consistently high, proving strong generalization.
- Minimal fluctuation in accuracy, suggesting stable training convergence.
- Model reached **100% accuracy** on validation data, indicating strong feature learning.
